
# [ê¹€ì„œì˜] spark local ì‘ì—… í™˜ê²½ êµ¬ì¶•

## Introduction

í•´ë‹¹ ê¸€ì€ [spark ë¡œì»¬ ì‘ì—… í™˜ê²½ ë ˆí¬](https://github.com/brickstudy/infra-docs/tree/main/spark) ë¥¼ ì•ˆë‚´í•˜ëŠ” ê¸€ì…ë‹ˆë‹¤. í•´ë‹¹ ì‘ì—… í™˜ê²½ì„ êµ¬ì„±í•˜ëŠ”ë°ì— ê³ ë ¤í•œ ë°°ê²½ ì§€ì‹ì„ ê°„ë‹¨íˆ ì •ë¦¬í•˜ì—¬ ê°™ì€ contextë¥¼ ê³µìœ í•˜ê³ , ê°œë°œí•œ ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ì†Œê°œí•˜ì—¬ ë°œì „ ë°©í–¥ ë° í”¼ë“œë°±ì„ ìˆ˜ë ´í•˜ê³ ì í•©ë‹ˆë‹¤.


## Table of Contents

1. [Introduction](#introduction)
2. [Background](#background)
    1. [Spark?](###ğŸ£1.-Spark?)
    2. [Why Spark? ](###ğŸ¥2.-why-spark?)
    3. [pandas vs spark](###ğŸ‘¯â€â™€ï¸3.-pandas-vs-spark)
    4. [Spark component](###ğŸ‘©â€ğŸ‘§â€ğŸ‘¦-4.-Spark-Component)
    5. [Spark execution modes](###ğŸ‹ï¸â€â™€ï¸-5.-Spark-Execution-Modes)
3. [Main Content](#main-content)
    1. [main development environment](###main-development-environment)
    2. [ê°œë°œ í™˜ê²½ ë¶„ë¦¬](###ê°œë°œ-í™˜ê²½-ë¶„ë¦¬)
    3. [guide](###guide)
    4. [etcs, further](###Etcs,-Further)

<br>

## Background
 
### ğŸ£1. Spark?
Apache SparkëŠ” í†µí•© ì»´í“¨íŒ… ì—”ì§„ì••ë‚˜ë‹¤. í´ëŸ¬ìŠ¤í„° í™˜ê²½ì—ì„œ ë°ì´í„°ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§‘í•©ì´ê³ , í˜„ì¬ ê°€ì¥ í™œë°œí•˜ê²Œ ê°œë°œë˜ê³  ìˆëŠ” ë³‘ë ¬ ì²˜ë¦¬ ì—”ì§„ì…ë‹ˆë‹¤.
- ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ë„¤ ê°€ì§€ ì–¸ì–´(scala, java, python, r) ì§€ì›
- sql, streaming, mlì— ì´ë¥´ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œê³µ, ë‹¨ì¼ ë…¸íŠ¸ë¶ í™˜ê²½ë¶€í„° ìˆ˜ì²œ ëŒ€ ì„œë²„ë¡œ êµ¬ì„±ëœ í´ëŸ¬ìŠ¤í„°ê¹Œì§€ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì‹¤í–‰ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ë¹…ë°ì´í„° ì²˜ë¦¬ë¥¼ ì‰½ê²Œ ì‹œì‘í•´ì„œ upper bound ì—†ëŠ” í° ê·œëª¨ í´ëŸ¬ìŠ¤í„°ë¡œ í™•ì¥í•´ë‚˜ê°ˆ ìˆ˜ ìˆìŒ

> cluster computing <br> 
> ë³´í†µ ì»´í“¨í„°ë¼ í•¨ì€, ì±…ìƒ ìœ„ì— ë†“ì—¬ì ¸ìˆëŠ” ì¥ë¹„ í•œ ëŒ€ë¥¼ ë– ì˜¬ë¦½ë‹ˆë‹¤. ì´ í•œ ëŒ€ ì»´í“¨í„°ë¡œëŠ” ì—°ì‚°í•  ìˆ˜ ì—†ëŠ” ë°ì´í„° ì²˜ë¦¬ ì‘ì—…ì´ ìˆëŠ”ë°, ì»´í“¨í„° í´ëŸ¬ìŠ¤í„°ëŠ” ì—¬ëŸ¬ ì»´í“¨í„°ì˜ ìì›ì„ ëª¨ì•„ í•˜ë‚˜ì˜ ì»´í“¨í„°ì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ ì´ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì»´í“¨í„° í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•˜ê³ , í´ëŸ¬ìŠ¤í„°ì—ì„œ ì‘ì—…ì„ ì¡°ìœ¨í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ìŠ¤íŒŒí¬ëŠ” ì´ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ë°ì´í„° ì²˜ë¦¬ ì‘ì—…ì„ ê´€ë¦¬í•˜ê³  ì¡°ìœ¨í•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.

>computing engine<br>
>ìŠ¤íŒŒí¬ëŠ” computing engineì…ë‹ˆë‹¤. ì˜êµ¬ ì €ì¥ì†Œì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ì§€ ì•Šê³ , ì €ì¥ì†Œ ì‹œìŠ¤í…œì˜ ë°ì´í„°ë¥¼ ì—°ì‚°í•˜ëŠ” ì—­í• ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. ëŒ€ì‹  ë‹¤ì–‘í•œ ì €ì¥ì†Œë¥¼ ì§€ì›í•˜ì—¬ ë°ì´í„° ì €ì¥ ìœ„ì¹˜ì— ìƒê´€ì—†ì´ ì²˜ë¦¬ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.

### ğŸ¥2. why spark?
- Fast Processing - Resilient Distributed Dataset (RDD)ëŠ” immutableí•œ ë¶„ì‚° ê°ì²´ ì§‘í•©ì…ë‹ˆë‹¤. RDDì— ìˆëŠ” ê° ë°ì´í„°ì…‹ë“¤ì€ ë…¼ë¦¬ì ì¸ partitionë“¤ë¡œ ë‚˜ëˆ ì ¸ìˆê³ , ì´ ë‹¨ìœ„ë¡œ í´ëŸ¬ìŠ¤í„°ì˜ ë…¸ë“œë“¤ì— ë¶„ì‚°ë˜ê³ , ì—°ì‚°ì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.
- In-memory computing - ë°ì´í„°ê°€ RAMì— ì €ì¥ë˜ì–´ (cached) ë°ì´í„° ë¶„ì„ ì†ë„ê°€ ë»ë¦…ë‹ˆë‹¤.
- Flexibility - ì—¬ëŸ¬ ì–¸ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. (Scala, Python, R, Java)
- Fault tolerance - RDDë¥¼ í†µí•´ ë°ì´í„° ì‹ ë¢°ì„± ë° ë³µêµ¬ ê°€ëŠ¥ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
- Analytics - Advanced Analytics, Machine Learning, deep learningê³¼ ê°™ì€ advanced ë¶„ì„ì´ built-in MLlibë¥¼ í†µí•´ ê°€ëŠ¥í•©ë‹ˆë‹¤.


### ğŸ‘¯â€â™€ï¸3. pandas vs spark

ë¶„ì‚° ì²˜ë¦¬ê°€ ë˜ëŠ”ì§€, ë˜ì§€ ì•ŠëŠ”ì§€ì˜ ì°¨ì´!

pandasëŠ” íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ dataframe ê°ì²´ê°€ ë¶„ì‚° ì»´í“¨í„°ê°€ ì•„ë‹Œ ë‹¨ì¼ ì»´í“¨í„°ì— ì¡´ì¬í•˜ì§€ë§Œ, spark Datafrmaeì€ sparkì—ì„œ ì œê³µí•˜ëŠ” ëŒ€í‘œì ì¸ êµ¬ì¡°ì  apië¡œ, ì—¬ëŸ¬ ì»´í“¨í„°ì— ë¶„ì‚°ë˜ì–´ìˆìŠµë‹ˆë‹¤.

|Spark DataFrame|Pandas DataFrame|
|:---:|:---:|
|parallelization ì§€ì›|parallelization ì§€ì›í•˜ì§€ ì•ŠìŒ|
|ì—¬ëŸ¬ ë…¸ë“œì— ê±¸ì³ ì‹¤í–‰|ë‹¨ì¼ ë…¸ë“œì—ì„œ ì‹¤í–‰|
|Lazy Evaluation ë”°ë¦„|Eager Execution ë”°ë¦„|
|immutable|mutable|
|ìƒëŒ€ì ìœ¼ë¡œ ë³µì¡í•œ operationì„ ìˆ˜í–‰í•˜ê¸° ì–´ë ¤ì›€|ìƒëŒ€ì ìœ¼ë¡œ ìˆ˜ì›”í•¨|
|ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ì²˜ë¦¬ ì†ë„ê°€ ë¹ ë¦„|ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ì†ë„ê°€ ë” ëŠë¦¼|
|scalableí•œ application ê°œë°œí•˜ê¸°ì— íƒì›”|scalable ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì— ì‚¬ìš© ë¶ˆê°€|
|fault tolerant|fault tolerance ë³´ì¥ ë¶ˆê°€|

>fault tolerant
>Sparkì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ ë°ì´í„° ì¶”ìƒí™” ë‹¨ìœ„ì¸ RDDì˜ ë¶„ì‚° ë³µì œ ì €ì¥, ë³€í™˜ ì‘ì—… ì¶”ì  Lineage Graph, checkpointingì„ í†µí•´ ë°ì´í„° ì†ì‹¤ ì‹œ ë³µêµ¬ë¥¼ ë³´ì¥í•˜ëŠ” ê°œë…

### ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ 4. Spark Component

spark applicationì€ driver process ì™€ ë‹¤ìˆ˜ì˜ executor processesë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

ğŸ” driver process
- í´ëŸ¬ìŠ¤í„° ë…¸ë“œ ì¤‘ **í•˜ë‚˜**ì—ì„œ ì‹¤í–‰ë¨
- **main() í•¨ìˆ˜** ì‹¤í–‰
- spark application ìˆ˜ëª… ì£¼ê¸° ë™ì•ˆ ê´€ë ¨ ëª¨ë“  ì •ë³´ ìœ ì§€ ë°€ ê´€ë¦¬
- ì‚¬ìš©ì í”„ë¡œê·¸ë¨/ì…ë ¥ ì‘ë‹µ, executor processì˜ job ê´€ë ¨ ë¶„ì„, ë°°í¬, ìŠ¤ì¼€ì¤„ë§

ğŸ¤ executor process
- í´ëŸ¬ìŠ¤í„°ì˜ **ì—¬ëŸ¬ ë¨¸ì‹ **ì—ì„œ ì‹¤í–‰ë˜ëŠ” ì‘ì—… ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤
- ë“œë¼ì´ë²„ í”„ë¡œì„¸ìŠ¤ê°€ í• ë‹¹í•œ **ì‘ì—…(ì½”ë“œ) ì‹¤í–‰**, ì§„í–‰ ìƒí™© ë“œë¼ì´ë²„ ë…¸ë“œì— ë³´ê³ 
- ê° ìµìŠ¤íí„°ëŠ” í´ëŸ¬ìŠ¤í„° ë…¸ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” JVM(Java Virtual Machine) í”„ë¡œì„¸ìŠ¤ë¡œ, ë©”ëª¨ë¦¬ì™€ CPUë¥¼ í• ë‹¹ë°›ì•„ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬

ë‘ í”„ë¡œì„¸ìŠ¤ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ ìì›ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ Cluster managerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Cluster managerëŠ” ë¬¼ë¦¬ì  ë¨¸ì‹ ì„ ê´€ë¦¬í•˜ê³  ìŠ¤íŒŒí¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ìì›ì„ í• ë‹¹í•˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ë¡œ, standalone, Hadoop Yarn, Mesos, Kubernetes ê°€ ìˆìŠµë‹ˆë‹¤. 

### ğŸ‹ï¸â€â™€ï¸ 5. Spark Execution Modes

#### 1. shell mode - interactive mode. direct manipulation in a shell (`spark-shell`, `pyspark`)
#### 2. local mode 
- non-distributed single JVM deployment mode. pseudo cluster
- parallelism defined by paramenter in a spark master url is the numner of threads
- simplt setup, get immediate feedback, cluster management is not required
- development, testing, debugging purpose

![240630_fig1-2](https://github.com/brickstudy/Docs/assets/52881652/3939603f-ca18-4813-b1f6-5d19f91a176c)

#### 3. cluster mode 
- cluster mode is for connecting into private network with several machines. 
- deploying on a cluster, running via cluster manager by submitting application
- `./bin/spark-submit --class <main-class> --master <master-url> --deploy-mode <deploy-mode> --conf <key>=<value> ... other options <application-jar> [application-arguments]`

![240630_fig3](https://github.com/brickstudy/Docs/assets/52881652/d3f075a0-e17f-4408-ad3b-c18488233969)

- there's two kinds of deploy mode : client mode, cluster mode
- 3-1. client mode (default)
    - driver **runs in a same process** as client that submits the app
    - driver program runs on a client machine **outside** the cluster (local machine from which the user submits the job)
    - users can directly check the driver logs on their local machine
    - development, debugging purpose
- 3-2. cluster mode
    - driver launched from a worker process
    - client process exits immediately after application submission
    - driver programì´ cluster ë‚´ nodes ì¤‘ í•˜ë‚˜ì—ì„œ ì‹¤í–‰ë¨ ì¦‰, cluster managerê°€ driver program ì‹¤í–‰í•  ë…¸ë“œë¥¼ ê²°ì •í•¨.
    - driver program runs on one of the nodes within the cluster, meaning that cluster manager decides on which node the driver program will execute
    - reduce network latency and allows for better resource management
    - driver logs must be checked on a node within the cluster
    - for production, huge data processing

![240630_fig4](https://github.com/brickstudy/Docs/assets/52881652/71aa73aa-57ed-4afc-be6b-80041f0f32c4)

## Main Content


### main development environment

í•´ë‹¹ ë¡œì»¬ ì‘ì—… í™˜ê²½ì€ dockerë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œì¸ ë©íƒ‘ì—ì„œ íŒ€ì› ëª¨ë‘ ê³µí†µëœ ê°œë°œ í™˜ê²½ì„ ì‰½ê²Œ ì…‹íŒ…í•  ìˆ˜ ìˆê²Œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.
ê³µí†µëœ ì‘ì—… í™˜ê²½ì€ íŒ€ ë‚´ì—ì„œ ì¼ê´€ë˜ê³  í‘œì¤€í™”ëœ í™˜ê²½ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í•„ìš”í•œ ë„êµ¬ ë° ìì›ì„ ëª…ì‹œì ì´ê³  ê³µí†µë˜ê²Œ ì •ì˜í•  ìˆ˜ ìˆê³ , ë¬¸ì œ í•´ê²°ê³¼ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì½”ìŠ¤íŠ¸ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.
databricksì—ì„œ ì œê³µí•˜ëŠ” í™˜ê²½ì˜ ê²½ìš° ë¹„ìš©ì´ ìƒë‹¹í•˜ê²Œ ë°œìƒí•˜ê³  community editionì˜ ê²½ìš° ì œê³µí•œ í´ëŸ¬ìŠ¤í„°ë¥¼ 2ì‹œê°„ ì£¼ê¸°ë¡œ íšŒìˆ˜í•˜ê¸° ë•Œë¬¸ì—, í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œ ê°œë°œí•˜ëŠ” ìƒí™©ì— ì í•©í•œ í™˜ê²½ì„ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆìŠµë‹ˆë‹¤.

ê³µí†µ ë¡œì»¬ ì‘ì—… í™˜ê²½ì€ í¬ê²Œ 0. Python 1. ëŒ€í™”í˜•ìœ¼ë¡œ ì‰½ê²Œ ì‘ì—…í•  ìˆ˜ ìˆëŠ” Jupyter Notebook 2. ë©”ì¸ ë°ì´í„° ì²˜ë¦¬ ì—”ì§„ Spark 3. AWS í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì—°ê²° ì„ dockerize í•˜ì—¬ ë°°í¬í•´ë‘ì—ˆìŠµë‹ˆë‹¤.
í•´ë‹¹ ë² ì´ìŠ¤ ì´ë¯¸ì§€ëŠ” ubuntuì— python, conda, jupyter, scipy package, spark ìˆœì„œ ë ˆì´ì–´ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. (ë„ì»¤ ì´ë¯¸ì§€ ê³„ì¸µêµ¬ì¡° figure6 ì°¸ê³ )

![240630_fig6](https://github.com/brickstudy/Docs/assets/52881652/f10b4712-e78f-477c-aee1-644a2b98ac87)



### ê°œë°œ í™˜ê²½ ë¶„ë¦¬

A. (Local mode) ë©”ì¸ í™˜ê²½! Notebookì—ì„œ ìŠ¤íŒŒí¬ë¥¼ ì´ìš©í•œ ë°ì´í„° ì²˜ë¦¬ ì½”ë“œë¥¼ interactiveí•˜ê²Œ ì‹¤í–‰í•˜ê³  ì‹¤ìŠµí•˜ê¸° ìœ„í•œ í™˜ê²½<br>
B. (Cluster mode - standalone) spark-submitìœ¼ë¡œ ìŠ¤í¬ë¦½íŠ¸ ì œì¶œí•˜ì—¬ spark jobì„ ì‹¤í–‰ì‹œí‚¤ëŠ” í™˜ê²½

![240630_fig5](https://github.com/brickstudy/Docs/assets/52881652/5b96eb36-79fc-4e6b-8693-35263f4f0b12)

í™˜ê²½ ë¶„ë¦¬ context

ì´ˆê¸° ì»´í¬ì¦ˆ íŒŒì¼ ì˜ ê²½ìš° master, worker spark-clusterì™€ ì£¼í”¼í„° ì»¨í…Œì´ë„ˆë¥¼ ë„ì›Œì„œ ì£¼í”¼í„° ìƒì—ì„œ spark-cluster ë‚´ì˜ ìŠ¤íŒŒí¬ ì—”ì§„ì„ standalone cluster modeë¡œ ì‚¬ìš©í•˜ê²Œë” ì„¤ì •ì´ ë˜ì–´ìˆìŠµë‹ˆë‹¤. 

í•˜ì§€ë§Œ jupyterlab ì»¨í…Œì´ë„ˆ ìì²´ì—ë„ spark ê°€ ì„¤ì¹˜ë˜ì–´ìˆê¸° ë•Œë¬¸ì—, ë…¸íŠ¸ë¶ìœ¼ë¡œ ì‹¤ìŠµí•˜ëŠ” ìš©ë„ë¡œ í™˜ê²½ì„ êµ¬ì„±í•˜ëŠ”ë°ì— spark-master, spark-worker ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•­ìƒ í•¨ê»˜ ë¹Œë“œí•˜ëŠ”ê²Œ ë¶ˆí•„ìš”í•˜ë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¨ìˆœ pyspark ì½”ë“œê°€ ì‹¤í–‰ë˜ëŠ”ì§€ ì‹¤ìŠµí• ë•Œì—ëŠ” jupyterlab ì»¨í…Œì´ë„ˆë§Œ ë„ì›Œ local mode(no cluster manager) ë¡œ í•™ìŠµ ë° ê°œë°œì´ ì§„í–‰ë˜ê³ , í´ëŸ¬ìŠ¤í„°ì˜ ê²½ìš° ë³„ë„ë¡œ(docker-compose_prod_test.yml) ë„ìš¸ ìˆ˜ ìˆê²Œë” ë¶„ë¦¬í•´ë‘ì–´ ì‹¤ì œ databricks í´ëŸ¬ìŠ¤í„° ìƒì— ì œì¶œí•˜ê¸° ì „ local clusterë¥¼ ë„ì›Œ ì‘ì—… ì œì¶œì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” í™˜ê²½ìœ¼ë¡œ ë¶„ë¦¬í–ˆìŠµë‹ˆë‹¤. 


A. jupyter( + spark)
- ë‹¨ìˆœ ê°„ë‹¨í•˜ê²Œ ìŠ¤íŒŒí¬ ì½”ë“œ ì‹¤í–‰í•´ë³¼ ìˆ˜ ìˆëŠ” í™˜ê²½
- Dockerfile, docker-compose.yml
- setup command : `docker-compose up -d`

B. spark master, spark worker, spark history server
- clusterì— ì¡ì„ ì œì¶œí•˜ê³  DAG, task schedule ë“± í™•ì¸í•  ìˆ˜ ìˆëŠ” í™˜ê²½
- Dockerfile_prod_test, docker-compose_prod_test.yml
- setup command : `docker-compose -f docker-compose_prod_test.yml up`

<br>

### AWS configuration

aws clië¥¼ ì»¨í…Œì´ë„ˆ ì•ˆì— ì„¤ì¹˜í•˜ê³ , spark ì™€ aws s3ë¥¼ ì—°ê²°í•˜ì—¬ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ëŠ”ê±¸ ì •ìƒì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤.

aws clië¥¼ ì‚¬ìš©í•˜ì—¬ aws í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„  ê° profileë§ˆë‹¤  AWS Access Key ID, AWS Secret Access Key, Default region name, Default output format (optional)ë¥¼ ëª…ì‹œí•´ì£¼ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì—, ì»¨í…Œì´ë„ˆ ë² ì´ìŠ¤ os unixê³„ì—´ ê¸°ë³¸ cliê°€ referí•˜ê³ ìˆëŠ” ê²½ë¡œ ~/.aws í•˜ì— config, credential íŒŒì¼ë¡œ í‚¤ ê°’ì„ ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤. (í˜„ì¬ ê°œì¸ ê³„ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸) aws cliëŠ” pip í†µí•´ì„œ ì„¤ì¹˜ë©ë‹ˆë‹¤. (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì‰˜ì—ì„œ `aws --version` `aws s3 ls` ë“±ì˜ ì»¤ë§¨ë“œ ì •ìƒ ì‘ë™ í™•ì¸ ì™„ë£Œ)

spark sessionì— aws s3 ì—°ê²° ê´€ë ¨ ê¸°ë³¸ êµ¬ì„±ì˜ ê²½ìš° conf/spark-defaults.conf íŒŒì¼ë¡œ ê´€ë¦¬ ê°€ëŠ¥í•˜ê²Œë” ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ aws connectorë¡œ ì‚¬ìš©ë˜ëŠ” hadoop-aws Jar, aws-java-sdk-bundle Jar íŒŒì¼ì˜ ê²½ìš° ìŠ¤íŒŒí¬ ì„¸ì…˜ ì‹œë§ˆë‹¤ maven í†µí•´ì„œ ë°›ì•„ì˜¤ê²Œ ì„¤ì • ì‹œ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë ¤ ë‹¤ìš´ë°›ì•„ì„œ ì´ë¯¸ì§€ ìì²´ jar ê²½ë¡œì— ìœ„ì¹˜ì‹œì¼°ìŠµë‹ˆë‹¤.

í•´ë‹¹ êµ¬ì„±ë“¤ì€ ë„ì»¤ ì´ë¯¸ì§€ë¡œ ë¹Œë“œí•˜ì—¬ dockerhubì— ì—…ë¡œë“œí•´ë‘ì—ˆê¸° ë•Œë¬¸ì—, ê°œì¸ ì‘ì—… í™˜ê²½ì—ì„œëŠ” docker-compose up ì»¤ë§¨ë“œë¥¼ í†µí•´ì„œ ê³µí†µëœ ê°œë°œ í™˜ê²½ì„ ì…‹ì—…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


<br>

### guide 

```bash
git clone https://github.com/brickstudy/infra-docs.git

cd spark && docker-compose up -d
```

ì›¹ ë¸Œë¼ìš°ì € í†µí•´ localhost:8888 ì ‘ê·¼ - jupyter í™˜ê²½

```python
from pyspark.sql import SparkSession
import os

# spark session ìƒì„± ì˜ˆì œ
spark = SparkSession.builder\
.appName("spark-local-environment-test")\
.config('spark.jars', '$SPARK_HOME/jars/hadoop-aws-3.2.0.jar')\
.config('spark.jars', '$SPARK_HOME/jars/aws-java-sdk-bundle-1.11.375.jar')\
.getOrCreate()

spark

# aws ì—°ê²° ì˜ˆì‹œ
s3_uri = "s3a://<uri-to-s3-bucket>"
df = spark.read.format('json').load(os.path.join(s3_uri, 'data/*.json.gz'))
df.show() 
df.printSchema() # basic print command of spark dataframe
display(df)      # enhance readability

```

<br>

### Etcs, Further

ì´ë¯¸ì§€ ìµœì í™”
- í˜„ì¬ jupyterì—ì„œ ì œê³µí•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, customizingì— í•œê³„ê°€ ìˆê³ , ë¶ˆí•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì´ë¯¸ì§€ì— í•¨ê¼ ë¹Œë“œë˜ì–´ìˆê±°ë‚˜ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ubuntu baseì´ê¸° ë•Œë¬¸ì—, ê²½ëŸ‰í™” ë¦¬ëˆ…ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ì´ë¯¸ì§€ í¬ê¸°ê°€ 8-9Gë¡œ ë‹¤ì†Œ í½ë‹ˆë‹¤.
- base distributed linux os(debian, ubuntu, centos, ..) + python + team configuration ì„ ê°€ì¥ ìƒìœ„ ë² ì´ìŠ¤ ì´ë¯¸ì§€ë¡œ ì„¤ì •í•˜ê³ , í•„ìš”í•œ í™˜ê²½ ìŠ¤í™ì„ ë…¼ì˜í•˜ì—¬ customed imageë¥¼ ë¹Œë“œí•œë‹¤ë©´ ê²½ëŸ‰í™”ëœ ì´ë¯¸ì§€ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í•´ë‹¹ ë² ì´ìŠ¤ ì´ë¯¸ì§€ì— jvm ë° spark ë¥¼ êµ¬ì¶•í•˜ë©´ ë³´ë‹¤ ê¹”ë”í•˜ê²Œ ì´ë¯¸ì§€ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“­ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ê²½ìš° osê°€ ë‹¬ë¼ì§ì— ë”°ë¼ ì˜ì¡´ì„±/í˜¸í™˜ì„±ì— ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆê³ , í…ŒìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.
<img width="523" alt="240630_fig7" src="https://github.com/brickstudy/Docs/assets/52881652/ecd30e29-d4c6-4552-96ae-b98f420e1669">
